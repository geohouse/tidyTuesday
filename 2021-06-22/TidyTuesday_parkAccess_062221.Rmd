---
title: "Tidy Tuesday June 22 2021"
author: "Geoffrey House"
date: "6/24/2021"
output: html_document
---

```{r download data}

# Get the Data

rm(list = ls())

# Read in with tidytuesdayR package 
# Install from CRAN via: install.packages("tidytuesdayR")
# This loads the readme and all the datasets for the week of interest

# Either ISO-8601 date or year/week works!

tuesdata <- tidytuesdayR::tt_load('2021-06-22')
#tuesdata <- tidytuesdayR::tt_load(2021, week = 26)

parks <- tuesdata$parks

# Or read in the data manually

#parks <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-06-22/parks.csv')


```

```{r data cleaning}
library(tidyverse)
library(pdftools)

raw_pdf <- pdftools::pdf_text("https://parkserve.tpl.org/mapping/historic/2020_ParkScoreRank.pdf")

# This is for page 1 of the pdf
raw_text <- raw_pdf[[1]] %>% 
  str_split("\n") %>% 
  unlist()

# The '.' means the current data. This is identical to the option below that subsets before piping
table_trimmed <- raw_text %>% 
  .[13:(length(raw_text)-1)] %>% 
  str_trim()


# table_trimmed2 <- raw_text[13:(length(raw_text)-1)] %>% 
#   str_trim()
# 
# identical(table_trimmed, table_trimmed2)

all_col_names <- c(
  "rank",
  "city",
  "med_park_size_data",
  "med_park_size_points",
  "park_pct_city_data",
  "park_pct_city_points",
  "pct_near_park_data",
  "pct_near_park_points",
  "spend_per_resident_data",
  "spend_per_resident_points",
  "basketball_data",
  "basketball_points",
  "dogpark_data",
  "dogpark_points",
  "playground_data",
  "playground_points",
  "rec_sr_data",
  "rec_sr_points",
  "restroom_data",
  "restroom_points",
  "splashground_data",
  "splashground_points",
  "amenities_points",
  "total_points",
  "total_pct",
  "city_dup"
)

# Make fixed width file out of the data and the column names
tab_names <- fwf_empty(
  table_trimmed,
  col_names = all_col_names
)

park_2020_1 <- table_trimmed %>% 
  read_fwf(
    tab_names
  ) 

# This is for page 2 of the pdf
# Replace multiple spaces with pipes, then use pipes as delimiters to read in the data
park_2020_2 <- raw_pdf[[2]] %>% 
  str_split("\n") %>% 
  unlist() %>% 
  .[1:41] %>% 
  str_trim() %>% 
  str_replace_all("\\s{2,}", "|") %>% 
  read_delim(
    delim = "|", 
    col_names = all_col_names
  )

# Combine data from both pages together
all_2020 <- bind_rows(park_2020_1, park_2020_2) 

raw_pdf_19 <- pdftools::pdf_text("https://parkserve.tpl.org/mapping/historic/2019_ParkScoreRank.pdf")

raw_pdf_19[[1]] %>% 
  str_split("\n") %>% 
  unlist() %>% 
  .[13:53] %>% 
  str_trim() %>% 
  str_replace_all("\\s{2,}", "|") %>%
  str_replace_all("% ", "|") %>% 
  read_delim(
    delim = "|", 
    col_names = FALSE
  ) %>% 
  set_names(all_col_names[str_detect(all_col_names, "total_pct", negate = TRUE)]) %>% 
  glimpse()

park_2019_2 <- raw_pdf_19[[2]] %>% 
  str_split("\n") %>% 
  unlist() %>% 
  .[1:44] %>% 
  str_trim() %>% 
  str_replace_all("\\s{2,}", "|") %>%
  str_replace_all("% ", "|") %>% 
  read_delim(
    delim = "|", 
    col_names = FALSE
  ) %>% 
  set_names(all_col_names[str_detect(all_col_names, "total_pct", negate = TRUE)]) %>% 
  glimpse()

# Function to reproducibly read and clean data from past years
read_and_clean <- function(year, page2 = TRUE){
  
  raw_pdf_in <- pdftools::pdf_text(glue::glue("https://parkserve.tpl.org/mapping/historic/{year}_ParkScoreRank.pdf"))
  
  df1 <- raw_pdf_in[[1]] %>% 
    str_split("\n") %>% 
    unlist() %>% 
    # .[range1] %>% 
    str_trim() %>% 
    str_subset("^[[:digit:]]+ ") %>% 
    str_subset("Ranking|ParkScore", negate = TRUE) %>% 
    str_replace_all("\\s{2,}", "|") %>%
    str_replace_all("% ", "|") %>% 
    read_delim(
      delim = "|", 
      col_names = FALSE
    ) 
  
  if(isTRUE(page2)){
      df2 <- raw_pdf_in[[2]] %>% 
        str_split("\n") %>% 
        unlist() %>% 
        # .[range2] %>% 
        str_trim() %>% 
        str_subset("^[[:digit:]]+ ") %>% 
        str_subset("Ranking|ParkScore", negate = TRUE) %>% 
        str_replace_all("\\s{2,}", "|") %>%
        str_replace_all("% ", "|") %>% 
        read_delim(
          delim = "|", 
          col_names = FALSE
        ) 
      
      bind_rows(df1, df2)
    } else {
      df1
    }
     
    }

all_2020 <- read_and_clean(2020) %>% 
  set_names(nm = all_col_names) %>% 
  mutate(year = 2020)
all_2019 <- read_and_clean(2019) %>% 
  set_names(all_col_names[str_detect(all_col_names, "total_points", negate = TRUE)]) %>% 
  mutate(year = 2019) %>% rename("total_points" = "total_pct")
all_2018 <- read_and_clean(2018) %>% 
  set_names(nm = all_col_names) %>% 
  mutate(year = 2018)
all_2017 <- read_and_clean(2017) %>% 
  set_names(nm = all_col_names[c(1:18, 23:26)]) %>% 
  rename(park_benches = total_pct) %>% 
  mutate(year = 2017)
all_2016 <- read_and_clean(2016) %>% 
  set_names(nm = c(all_col_names[c(1:18, 23:26)], "city_dup2")) %>% 
  rename(park_benches = city_dup, city_dup = city_dup2) %>% 
  mutate(year = 2016)
all_2015 <- read_and_clean(2015, FALSE) %>% 
  set_names(nm = c(all_col_names[c(1:18, 23:25)], "park_benches")) %>% 
  mutate(year = 2015) %>% rename("orig_total_points" = "total_points", "total_points" = "total_pct")
all_2014 <- read_and_clean(2014, FALSE) %>% 
  set_names(nm = c(all_col_names[c(1:10, 15:16, 25)], "park_benches")) %>% 
  mutate(year = 2014) %>% rename("total_points" = "total_pct")
all_2013 <- read_and_clean(2013, FALSE) %>% 
  set_names(nm = c(all_col_names[c(1:10, 15:16, 24:25)], "park_benches")) %>% 
  mutate(year = 2013)
all_2012 <- read_and_clean(2012, FALSE) %>% 
  separate(X1, c("rank", "city"), extra = "merge") %>% 
  mutate(rank = as.double(rank)) %>% 
  set_names(nm = c(all_col_names[c(1:10, 15:16, 24:25)], "park_benches")) %>% 
  mutate(year = 2012)

all_data <- bind_rows(list(all_2020, all_2019, all_2018, all_2017, all_2016, all_2015, all_2014, all_2013, all_2012)) %>% 
  select(year, everything())

all_data %>% 
  ggplot(aes(x = year, y = med_park_size_data, group = year)) +
  geom_boxplot()

all_data %>% glimpse()

all_data %>% 
  write_csv("parks.csv")

#update_data_type("parks.csv", ",")


```

```{r total_pct through time}
library(plotly)

# Data for year 2017 don't appear because there's no column named 'total_pct' for 2017 data.

total_pct_through_time <- all_data %>% ggplot(mapping = aes(x = year, y = total_pct, color = city)) + geom_point(show.legend = FALSE)

ggplotly(total_pct_through_time)

# Normalize the fractional point scores to the max score for each year, and convert the spend_per_resident_data column into numeric after dropping the '$'
all_data_add_pct <- all_data %>% group_by(year) %>% mutate(emp_pct = total_points/ max(total_points), spend_per_resident_data = as.numeric(gsub(pattern = "[$]",replacement = "",x = spend_per_resident_data)))

total_pct_through_time_2 <- all_data_add_pct %>% ggplot(mapping = aes(x = year, y = emp_pct, color = city)) + geom_point(show.legend = FALSE)

ggplotly(total_pct_through_time_2)

```

```{r score as function of spend}


score_spend <- all_data_add_pct %>% 
  ggplot(mapping = aes(x = spend_per_resident_data, y = emp_pct, label = city)) + geom_point() + facet_wrap(~year) + theme_bw() 

ggplotly(score_spend)



```


### Looking at the 2020 data in particular, it looks like there is diminishing returns for increasing scores with spending above $200 per resident. Exploring that more here.

```{r return on investment threshold}

# Try lm for 2020 below $200 spend compared to all cities. Looks like highly diminishing returns above $200 speding.

data_subset_2020 <- all_data_add_pct[which(all_data_add_pct$year == 2020),]

data_2020_plot <- data_subset_2020 %>% ggplot(mapping = aes(x = spend_per_resident_data, y = emp_pct)) + geom_point() + theme_bw() + geom_vline(xintercept = 200, color = "red")

data_2020_plot
```

#### If we look at linear models fit on the whole dataset and only fit for the cities with spending of at most $200 per resident, a linear model fits better when only cities with spending at most $200. This is the case when looking at just the adjusted R^2 values ... 
```{r roi threshold lm models}



lm_fullSet <- lm(data_subset_2020$emp_pct ~ data_subset_2020$spend_per_resident_data)

fullSet_r2 <- summary(lm_fullSet)$adj.r.squared

data_subset_2020_ltet200spend <- data_subset_2020[which(data_subset_2020$spend_per_resident_data <= 200),]

lm_ltet200spend <- lm(data_subset_2020_ltet200spend$emp_pct ~ data_subset_2020_ltet200spend$spend_per_resident_data)

ltet200spend_r2 <- summary(lm_ltet200spend)$adj.r.squared

knitr::kable(data.frame("adj_rSquare_allData" = fullSet_r2, "adj_rSquare_spendingAtMost200" = ltet200spend_r2))

```

#### This better model fit for the reduced model is also apparent when looking at the model residuals as a function of per capita spending. The residuals from the full model show a clear trend for cities with more spending, but are more randomly occurring in the model only considering cities with at most $200 per capita spending.

```{r roi fit models plots}
library(gridExtra)

plot_2020_all_fit <- data_subset_2020 %>% ggplot(mapping = aes(x = spend_per_resident_data, y = emp_pct)) + geom_point() + theme_bw() + geom_abline(intercept = lm_fullSet$coefficients[1], slope = lm_fullSet$coefficients[2], color = "red") + xlab("Per capita annual spending on parks ($)") + ylab("Fractional score for each \ncities' parks (1 is best)") + ggtitle("Linear model fit to all 2020 data")

# Add a marker vector for cities that have per captita spending of more than $200 annually and those that have less. The TRUE statement acts like if/else, to catch anything not set in the first test.
data_subset_2020 <- data_subset_2020 %>% mutate(`per capita break` = case_when(spend_per_resident_data <= 200 ~ "<= $200", TRUE ~ "> $200"))

plot_2020_ltet200_fit <- data_subset_2020 %>% ggplot(mapping = aes(x = spend_per_resident_data, y = emp_pct, color = `per capita break`)) + geom_point() + theme_bw() + geom_abline(intercept = lm_ltet200spend$coefficients[1], slope = lm_ltet200spend$coefficients[2], color = "red") + xlab("Per capita annual spending on parks ($)") + ylab("Fractional score for each \ncities' parks (1 is best)") + ggtitle("Linear model fit only for cities with <= $200 per capita spending") + scale_colour_manual(values=setNames(c("black", "light gray"), c("<= $200", "> $200")), guide = FALSE)

# Set up data frames with the residual values for easy plotting
resid_all_2020 <- data.frame("spend_per_resident" = data_subset_2020$spend_per_resident_data, "residuals" = lm_fullSet$residuals)

resid_ltet200_2020 <- data.frame("spend_per_resident" = data_subset_2020_ltet200spend$spend_per_resident_data, "residuals" = lm_ltet200spend$residuals)

plot_2020_all_resid <- resid_all_2020 %>% ggplot(mapping = aes(x = spend_per_resident, y = residuals)) + geom_point(pch = 21, fill = "blue") + theme_bw() + xlab("Per capita annual spending on parks ($)") + ylab("Residuals") + ggtitle("Residuals of linear model fit to all 2020 data")

plot_2020_ltet200_resid <- resid_ltet200_2020 %>% ggplot(mapping = aes(x = spend_per_resident, y = residuals)) + geom_point(pch = 21, fill = "blue") + theme_bw() + xlab("Per capita annual spending on parks ($)") + ylab("Residuals") + ggtitle("Residuals of linear model fit only for cities with <= $200 per capita spending")

grid.arrange(plot_2020_all_fit, plot_2020_ltet200_fit, plot_2020_all_resid, plot_2020_ltet200_resid, nrow = 2)

```

#### It's expected to have an asymptote to values where fractional values are measured, so its presence isn't surprising, but it's useful to see that the asymptote occurs around $200 per capita spending. This could potentially help guide overall city budgets to help maximize the accessibility and cultural importance of parks while balancing other civic priorities.

